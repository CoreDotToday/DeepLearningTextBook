{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb5626d3",
   "metadata": {},
   "source": [
    "# Keras를 이용해 seq2seq를 10분안에 알려주기  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d2d029",
   "metadata": {},
   "source": [
    "원문 : [A ten-minute introduction to sequence-to-sequence learning in Keras](https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html)   \n",
    "업데이트판 : [Character-level recurrent sequence-to-sequence model](https://keras.io/examples/nlp/lstm_seq2seq/)   \n",
    "번역 : [케라스를 이용해 seq2seq를 10분안에 알려주기](https://tykimos.github.io/2018/09/14/ten-minute_introduction_to_sequence-to-sequence_learning_in_Keras/)   \n",
    "ipynb : [코어닷투데이 github](http://github.com/coredottoday/DeepLearningTextBook/)\n",
    "\n",
    "> 본 문서는 케라스를 이용해 RNN(Recurrent Neural Networks)모델인 Seq2Seq를 10분 안에 알려주는 튜토리얼 한글 버전입니다. Seq2Seq의 의미부터 케라스를 이용한 모델 구현을 다루고 있으며 본 문서 대상자는 recurrent networks와 keras에 대한 경험이 있다는 가정하에 진행합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7863cce7",
   "metadata": {},
   "source": [
    "* Keras\n",
    "* RNN\n",
    "* LSTM\n",
    "* NLP\n",
    "* Seq2Seq\n",
    "* GRU layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d07891",
   "metadata": {},
   "source": [
    "#### sequence-to-sequence 학습이란?\n",
    "sequence-to-sequence(Seq2Seq) 학습은 한 도메인(예: 영어 문장)에서 다른 도메인(예: 불어로 된 문장)으로 시퀀스(sequence)를 변환하는 모델 학습을 의미합니다.\n",
    "\n",
    "```bash\n",
    "    \"the cat sat on the mat\" -> [Seq2Seq model] -> \"le chat etait assis sur le tapis\"\n",
    "``` \n",
    "\n",
    "이 모델은 기계 번역 혹은 자유로운 질의응답에 사용됩니다. (자연어 질문을 주어 자연어 응답을 생성) \n",
    "--일반적으로, 텍스트를 생성해야 할 경우라면 언제든지 적용할 수 있습니다.  \n",
    " \n",
    "해당 작업을 다루는 여러 가지 방법이(**RNN** 혹은 **1D convnets**) 있습니다.\n",
    "> 이번 문서에선 **RNN**을 사용하고 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c5d9fd",
   "metadata": {},
   "source": [
    "#### 자명한(명확한) 사례 : 입력과 출력 시퀀스 길이가 같을 때\n",
    "입력과 출력 시퀀스 길이가 같을 경우, 케라스 Long Short-Term Memory(LSTM)이나 GRU 계층(혹은 다수의 계층) 같은 모델들을 간단하게 구현할 수 있습니다. [예제 스크립트](https://github.com/keras-team/keras-io/blob/master/examples/nlp/addition_rnn.py)에선 어떻게 RNN으로 문자열로 인코딩된 숫자들에 대한 덧셈 연산을 학습할 수 있는지 보여주고 있습니다.\n",
    "\n",
    "![The trivial case](https://raw.githubusercontent.com/CoreDotToday/DeepLearningTextBook/main/Seq2Seq/static/s2s_1.png)\n",
    "\n",
    "이 방법의 주의점은 주어진 `input[...t]`으로 `target[...t]`을 생성 가능하다고 가정하는 것입니다. 일부 경우(예: 숫자된 문자열 추가)에선 정상적으로 작동하지만, 대부분의 경우에는 작동하지 않습니다. 일반적으론, 목표 시퀀스를 생성하기 위해 전체 입력 시퀀스 정보가 필요합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb060fab",
   "metadata": {},
   "source": [
    "#### 일반 사례 : 표준 sequence-to-sequence\n",
    "일반적으론 입력과 출력 시퀀스 길이가 다르고(예: 기계 번역) 목표 시퀀스를 예측하기 위해 전체 입력 시퀀스 정보가 필요합니다. 이를 위해 고급 설정이 필요하며, 일반적으로 \"Seq2Seq models\"를 언급할 때 참조합니다. 동작 방법은 하단을 참조하시면 되겠습니다.\n",
    "\n",
    "- 하나(혹은 여러 개)의 RNN 계층은 \"encoder\" 역할을 합니다 : 입력 시퀀스를 처리하고 자체 내부 상태를 반환합니다. 여기서, encoder RNN의 결과는 사용하지 않고 상태만 복구시킵니다. 이 상태가 다음 단계에서 decoder의 \"문맥\" 혹은 \"조건\" 역할을 합니다.\n",
    "- 또 하나(혹은 여러 개)의 RNN 계층은 \"decoder\" 역할을 합니다 : 목표 시퀀스에서 이전 문자들에 따라 다음 문자들을 예측하도록 훈련됩니다. 상세히 말하면, 목표 시퀀스를 같은 시퀀스로 바꾸지만 후에 \"teacher forcing\"이라는 학습 과정인, 한 개의 time step만큼 offset*이 되도록 훈련됩니다. 중요한 건, encoder는 encoder 상태 벡터들을 초기 상태로 사용하고 이는 decoder가 생성할 정보를 얻는 방법이기도 합니다. 사실, decoder는 주어진 `target[...t]`을 입력 시퀀스에 맞춰서 `target[t+1...]`을 생성하는 법을 학습합니다.\n",
    "\n",
    "> offset 의 예: 문자 A의 배열이 'abcdef'를 가질 때, 'c'가 A 시작점에서 2의 offset을 지님\n",
    "\n",
    "\n",
    "![seq2seq-teacher-forcing](https://raw.githubusercontent.com/CoreDotToday/DeepLearningTextBook/main/Seq2Seq/static/s2s_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33783751",
   "metadata": {},
   "source": [
    "추론 방식(즉: 알 수 없는 입력 시퀀스를 해석하려고 할 때)에선 약간 다른 처리를 거치게 됩니다.\n",
    "\n",
    "1. 입력 시퀀스를 상태 벡터들로 바꿉니다.\n",
    "2. 크기가 1인 목표 시퀀스로 시작합니다. (시퀀스의 시작 문자에만 해당)\n",
    "3. 상태 벡터들과 크기가 1인 목표 시퀀스를 decoder에 넣어 다음 문자에 대한 예측치를 생성합니다.\n",
    "4. 이런 예측치들을 사용해 다음 문자의 표본을 뽑습니다.(간단하게 argmax를 사용)\n",
    "5. 목표 시퀀스에 샘플링된 문자를 붙입니다.\n",
    "6. 시퀀스 종료 문자를 생성하거나 끝 문자에 도달할 때까지 앞의 과정을 반복합니다.\n",
    "\n",
    "![seq2seq-inference](https://raw.githubusercontent.com/CoreDotToday/DeepLearningTextBook/main/Seq2Seq/static/s2s_3.png)\n",
    "\n",
    "이같은 과정은 *\"teacher forcing\" 없이* Seq2Seq를 학습시킬 때 쓰일 수도 있습니다. (decoder의 예측치들을 decoder에 다시 기재함으로써)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fe3cd5",
   "metadata": {},
   "source": [
    "# 케라스 예제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22cdf1b",
   "metadata": {},
   "source": [
    "실제 코드를 통해 위의 아이디어들을 설명하겠습니다.\n",
    "\n",
    "예제를 구현하기 위해, 영어 문장과 이에 대해 불어로 번역한 문장 한 쌍으로 구성된 데이터 세트를 사용합니다. ([manythings.org/anki](http://www.manythings.org/anki/)에서 내려받을 수 있습니다.) 다운받을 파일은 `fra-eng.zip`입니다. 입력 문자를 문자 단위로 처리하고, 문자 단위로 출력문자를 생성하는 *문자 수준* Seq2Seq model을 구현할 예정입니다. 또 다른 옵션은 기계 번역에서 좀 더 일반적인 *단어 수준* model입니다. 글 끝단에서, Embedding계층을 사용하여 설명에 쓰인 model을 단어 수준 model로 바꿀 수 있는 참고 사항도 보실 수 있습니다.\n",
    "\n",
    "설명에 쓰인 예제 전체 code는 [Github](https://github.com/fchollet/keras/blob/master/examples/lstm_seq2seq.py)에서 보실 수 있습니다.\n",
    "\n",
    "진행 과정 요약:\n",
    "\n",
    "1. 문장들을 3차원 배열(`encoder_input_data`, `decoder_input_data`, `decoder_target_data`)로 변환합니다.\n",
    "    - `encoder_input_data`는 (`num_pairs`, `max_english_sentence_length`, `num_english_characters`) 형태의 3차원 배열로 영어 문장의 one-hot 형식 벡터 데이터를 갖고 있습니다.\n",
    "    - `decoder_input_data`는 (`num_pairs`, `max_french_sentence_length`, `num_french_characters`)형태의 3차원 배열로 불어 문장의 one-hot형식 벡터 데이터를 갖고 있습니다.\n",
    "    - `decoder_target_data`는 `decoder_input_data`와 같지만 *하나의 time step만큼 offset 됩니다.* `decoder_target_data[:, t, :]`는 `decoder_input_data[:, t + 1, :]`와 같습니다.  \n",
    "2. 기본 LSTM 기반의 Seq2Seq model을 주어진 `encoder_input_data`와 `decoder_input_data`로 `decoder_target_data`를 예측합니다. 해당 model은 teacher forcing을 사용합니다.\n",
    "3. model이 작동하는지 확인하기 위해 일부 문장을 디코딩(decoding)합니다. (`encoder_input_data`의 샘플을 `decoder_target_data`의 표본으로 변환합니다.)\n",
    "\n",
    "(문장을 디코딩하는)학습 단계와 추론 단계는 꽤나 다르기 때문에, 같은 내부 계층을 사용하지만 서로 다른 모델을 사용합니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded92cce",
   "metadata": {},
   "source": [
    "다음은 원문 저자가 제공하는 model로 keras RNN의 3가지 핵심 특징들을 사용합니다:\n",
    "- `return_state`는 encoder의 출력과 내부 RNN 상태인 리스트를 반환하도록 RNN을 구성하는 인수입니다. 이는 encoder의 상태를 복구하는 데 사용합니다.\n",
    "- `inital_state`는 RNN의 초기 상태를 지정하는 인수입니다. 초기 상태로 incoder를 decoder로 전달하는 데 사용합니다.\n",
    "- `return_sequences`는 출력된 전체 시퀀스를 반환하도록 구성하는 인수(마지막 출력을 제외하곤 기본 동작)로 decoder에 사용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5927251",
   "metadata": {},
   "source": [
    "# 라이브러리 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4dac87a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "65cae83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, LSTM, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203dbba1",
   "metadata": {},
   "source": [
    "# 데이터 준비하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2422641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 6300k  100 6300k    0     0  4091k      0  0:00:01  0:00:01 --:--:-- 4088k\n",
      "Archive:  fra-eng.zip\n",
      "  inflating: _about.txt              \n",
      "  inflating: fra.txt                 \n"
     ]
    }
   ],
   "source": [
    "!curl -O http://www.manythings.org/anki/fra-eng.zip\n",
    "!unzip fra-eng.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f481d831",
   "metadata": {},
   "source": [
    "# 설정 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30f7c24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64  # 학습할 배치 사이즈\n",
    "epochs = 100  # 학습할 epoch 수\n",
    "latent_dim = 256  # 인코딩 공간의 잠재 차원 크기(Latent dimensionality)\n",
    "num_samples = 10000  # 학습할 샘플의 개수\n",
    "data_path = \"fra.txt\"  # 데이터 파일의 경로"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421c12e7",
   "metadata": {},
   "source": [
    "# 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc5f7523",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 10000\n",
      "Number of unique input tokens: 71\n",
      "Number of unique output tokens: 92\n",
      "Max sequence length for inputs: 15\n",
      "Max sequence length for outputs: 59\n"
     ]
    }
   ],
   "source": [
    "# 데이터를 벡터화(Vectorize) 합니다\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.read().split(\"\\n\")\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text, _ = line.split(\"\\t\")\n",
    "    # 문장이 시작된다는 의미로 \"tab\"(\\t) 을 사용하겠습니다\n",
    "    # \"\\n\"은 문장이 종료된다는 캐릭터로 사용하겠습니다(end sequence)\n",
    "    target_text = \"\\t\" + target_text + \"\\n\"\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "print(\"샘플의 개수:\", len(input_texts))\n",
    "print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
    "print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
    "print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
    "print(\"Max sequence length for outputs:\", max_decoder_seq_length)\n",
    "\n",
    "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\"\n",
    ")\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
    ")\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
    ")\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.0\n",
    "    encoder_input_data[i, t + 1 :, input_token_index[\" \"]] = 1.0\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.0\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
    "    decoder_input_data[i, t + 1 :, target_token_index[\" \"]] = 1.0\n",
    "    decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b93eb8d",
   "metadata": {},
   "source": [
    "# 모델 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e6e9b135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 시퀀스의 정의와 처리\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# `encoder_outputs`는 버리고 상태(`state_h, state_c`)는 유지\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# `encoder_states`를 초기 상태로 사용해 decoder를 설정\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "\n",
    "# 전체 출력 시퀀스를 반환하고 내부 상태도 반환하도록 decoder를 설정. \n",
    "# 학습 모델에서 상태를 반환하도록 하진 않지만, inference에서 사용할 예정.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation=\"softmax\")\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# `encoder_input_data`와 `decoder_input_data`를 `decoder_target_data`로 반환하도록 모델을 정의\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "81669583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, None, 71)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, None, 92)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_6 (LSTM)                   [(None, 256), (None, 335872      input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_7 (LSTM)                   [(None, None, 256),  357376      input_8[0][0]                    \n",
      "                                                                 lstm_6[0][1]                     \n",
      "                                                                 lstm_6[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, None, 92)     23644       lstm_7[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 716,892\n",
      "Trainable params: 716,892\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a018286",
   "metadata": {},
   "source": [
    "# 모델 학습하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2062745a",
   "metadata": {},
   "source": [
    "밑의 2줄로 샘플의 20%를 검증 데이터 세트로 손실을 관찰하면서 모델을 학습시킵니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b807fee",
   "metadata": {},
   "source": [
    "1epoch 당 40s가 걸리니 100epoch에 4,000초 정도가 걸리겠네요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "54d896e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/khkim/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-19 07:30:13.127557: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2021-07-19 07:30:13.554592: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-07-19 07:30:13.555686: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
      "name: TITAN X (Pascal) major: 6 minor: 1 memoryClockRate(GHz): 1.531\n",
      "pciBusID: 0000:03:00.0\n",
      "2021-07-19 07:30:13.556911: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.0'; dlerror: libcudart.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda/lib64\n",
      "2021-07-19 07:30:13.557273: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcublas.so.10.0'; dlerror: libcublas.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda/lib64\n",
      "2021-07-19 07:30:13.557577: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcufft.so.10.0'; dlerror: libcufft.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda/lib64\n",
      "2021-07-19 07:30:13.557929: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcurand.so.10.0'; dlerror: libcurand.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda/lib64\n",
      "2021-07-19 07:30:13.558191: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusolver.so.10.0'; dlerror: libcusolver.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda/lib64\n",
      "2021-07-19 07:30:13.558543: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusparse.so.10.0'; dlerror: libcusparse.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda/lib64\n",
      "2021-07-19 07:30:13.558877: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda/lib64\n",
      "2021-07-19 07:30:13.558893: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1662] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2021-07-19 07:30:13.572776: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 1995355000 Hz\n",
      "2021-07-19 07:30:13.574233: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55df641eddf0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-07-19 07:30:13.574270: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-07-19 07:30:13.846297: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-07-19 07:30:13.848316: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55df641e2c60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2021-07-19 07:30:13.848358: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): TITAN X (Pascal), Compute Capability 6.1\n",
      "2021-07-19 07:30:13.848537: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-07-19 07:30:13.848557: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/khkim/anaconda3/envs/tf1/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 42s 5ms/step - loss: 1.1419 - accuracy: 0.7351 - val_loss: 1.0177 - val_accuracy: 0.7252\n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 40s 5ms/step - loss: 0.8245 - accuracy: 0.7766 - val_loss: 0.8383 - val_accuracy: 0.7699\n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 40s 5ms/step - loss: 0.6745 - accuracy: 0.8125 - val_loss: 0.7202 - val_accuracy: 0.7900\n",
      "Epoch 4/100\n",
      "8000/8000 [==============================] - 40s 5ms/step - loss: 0.5768 - accuracy: 0.8324 - val_loss: 0.6366 - val_accuracy: 0.8126\n",
      "Epoch 5/100\n",
      "8000/8000 [==============================] - 40s 5ms/step - loss: 0.5271 - accuracy: 0.8451 - val_loss: 0.5883 - val_accuracy: 0.8286\n",
      "Epoch 6/100\n",
      "8000/8000 [==============================] - 40s 5ms/step - loss: 0.4914 - accuracy: 0.8553 - val_loss: 0.5619 - val_accuracy: 0.8348\n",
      "Epoch 7/100\n",
      "8000/8000 [==============================] - 40s 5ms/step - loss: 0.4634 - accuracy: 0.8629 - val_loss: 0.5408 - val_accuracy: 0.8403\n",
      "Epoch 8/100\n",
      "8000/8000 [==============================] - 40s 5ms/step - loss: 0.4399 - accuracy: 0.8689 - val_loss: 0.5195 - val_accuracy: 0.8470\n",
      "Epoch 9/100\n",
      "8000/8000 [==============================] - 40s 5ms/step - loss: 0.4191 - accuracy: 0.8750 - val_loss: 0.5069 - val_accuracy: 0.8511\n",
      "Epoch 10/100\n",
      "8000/8000 [==============================] - 40s 5ms/step - loss: 0.4004 - accuracy: 0.8803 - val_loss: 0.4982 - val_accuracy: 0.8523\n",
      "Epoch 11/100\n",
      "8000/8000 [==============================] - 40s 5ms/step - loss: 0.3836 - accuracy: 0.8848 - val_loss: 0.4871 - val_accuracy: 0.8560\n",
      "Epoch 12/100\n",
      "8000/8000 [==============================] - 40s 5ms/step - loss: 0.3681 - accuracy: 0.8891 - val_loss: 0.4815 - val_accuracy: 0.8584\n",
      "Epoch 13/100\n",
      "8000/8000 [==============================] - 40s 5ms/step - loss: 0.3532 - accuracy: 0.8938 - val_loss: 0.4687 - val_accuracy: 0.8620\n",
      "Epoch 14/100\n",
      "8000/8000 [==============================] - 40s 5ms/step - loss: 0.3396 - accuracy: 0.8977 - val_loss: 0.4710 - val_accuracy: 0.8622\n",
      "Epoch 15/100\n",
      "8000/8000 [==============================] - 40s 5ms/step - loss: 0.3268 - accuracy: 0.9012 - val_loss: 0.4577 - val_accuracy: 0.8663\n",
      "Epoch 16/100\n",
      "8000/8000 [==============================] - 40s 5ms/step - loss: 0.3147 - accuracy: 0.9052 - val_loss: 0.4549 - val_accuracy: 0.8682\n",
      "Epoch 17/100\n",
      "8000/8000 [==============================] - 40s 5ms/step - loss: 0.3035 - accuracy: 0.9083 - val_loss: 0.4498 - val_accuracy: 0.8702\n",
      "Epoch 18/100\n",
      "8000/8000 [==============================] - 40s 5ms/step - loss: 0.2922 - accuracy: 0.9117 - val_loss: 0.4527 - val_accuracy: 0.8695\n",
      "Epoch 19/100\n",
      "8000/8000 [==============================] - 40s 5ms/step - loss: 0.2820 - accuracy: 0.9150 - val_loss: 0.4534 - val_accuracy: 0.8697\n",
      "Epoch 20/100\n",
      "8000/8000 [==============================] - 40s 5ms/step - loss: 0.2727 - accuracy: 0.9175 - val_loss: 0.4458 - val_accuracy: 0.8721\n",
      "Epoch 21/100\n",
      "8000/8000 [==============================] - 40s 5ms/step - loss: 0.2630 - accuracy: 0.9200 - val_loss: 0.4486 - val_accuracy: 0.8721\n",
      "Epoch 22/100\n",
      "8000/8000 [==============================] - 40s 5ms/step - loss: 0.2540 - accuracy: 0.9232 - val_loss: 0.4506 - val_accuracy: 0.8729\n",
      "Epoch 23/100\n",
      "8000/8000 [==============================] - 40s 5ms/step - loss: 0.2454 - accuracy: 0.9255 - val_loss: 0.4481 - val_accuracy: 0.8737\n",
      "Epoch 24/100\n",
      "8000/8000 [==============================] - 40s 5ms/step - loss: 0.2370 - accuracy: 0.9281 - val_loss: 0.4522 - val_accuracy: 0.8742\n",
      "Epoch 25/100\n",
      "8000/8000 [==============================] - 40s 5ms/step - loss: 0.2296 - accuracy: 0.9300 - val_loss: 0.4532 - val_accuracy: 0.8744\n",
      "Epoch 26/100\n",
      "8000/8000 [==============================] - 40s 5ms/step - loss: 0.2222 - accuracy: 0.9322 - val_loss: 0.4578 - val_accuracy: 0.8745\n",
      "Epoch 27/100\n",
      "8000/8000 [==============================] - 40s 5ms/step - loss: 0.2151 - accuracy: 0.9345 - val_loss: 0.4589 - val_accuracy: 0.8750\n",
      "Epoch 28/100\n",
      "8000/8000 [==============================] - 40s 5ms/step - loss: 0.2084 - accuracy: 0.9364 - val_loss: 0.4597 - val_accuracy: 0.8744\n",
      "Epoch 29/100\n",
      "8000/8000 [==============================] - 40s 5ms/step - loss: 0.2021 - accuracy: 0.9383 - val_loss: 0.4649 - val_accuracy: 0.8745\n",
      "Epoch 30/100\n",
      "8000/8000 [==============================] - 40s 5ms/step - loss: 0.1958 - accuracy: 0.9403 - val_loss: 0.4663 - val_accuracy: 0.8744\n",
      "Epoch 31/100\n",
      "8000/8000 [==============================] - 40s 5ms/step - loss: 0.1897 - accuracy: 0.9419 - val_loss: 0.4695 - val_accuracy: 0.8754\n",
      "Epoch 32/100\n",
      "8000/8000 [==============================] - 40s 5ms/step - loss: 0.1839 - accuracy: 0.9437 - val_loss: 0.4849 - val_accuracy: 0.8730\n",
      "Epoch 33/100\n",
      "8000/8000 [==============================] - 40s 5ms/step - loss: 0.1788 - accuracy: 0.9454 - val_loss: 0.4814 - val_accuracy: 0.8738\n",
      "Epoch 34/100\n",
      "2560/8000 [========>.....................] - ETA: 24s - loss: 0.1701 - accuracy: 0.9484"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_349361/3038957006.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Save model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf1/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/tf1/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3476\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "model.fit(\n",
    "    [encoder_input_data, decoder_input_data],\n",
    "    decoder_target_data,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.2,\n",
    ")\n",
    "\n",
    "# 모델을 저장합니다\n",
    "# 다음 번에는 fit 하는 과정 없이 저장한 모델 파일만 불러와서 추론하셔도 됩니다!\n",
    "model.save(\"s2s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e51cbb",
   "metadata": {},
   "source": [
    "# 추론하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c2b93f",
   "metadata": {},
   "source": [
    "맥북 CPU에서 1시간 정도 학습한 후에, 추론할 준비가 됩니다. 테스트 문장을 decode하기 위해 반복 수행할 것입니다.\n",
    "\n",
    "1. 입력문장을 encode하고 초기 상태에 decoder의 상태를 가지고 옵니다.\n",
    "2. 초기 상태 decoder의 한 단계와 \"시퀀스 시작\" 토큰을 목표로 실행합니다. 출력은 다음 목표 문자입니다.\n",
    "3. 예측된 목표 문자를 붙이고 이를 반복합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa11180",
   "metadata": {},
   "source": [
    "다음은 추론을 설정한 부분입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "081ea8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sampling models\n",
    "# Restore the model and construct the encoder and decoder.\n",
    "model = load_model(\"s2s\")\n",
    "\n",
    "encoder_inputs = model.input[0]  # input_1\n",
    "encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output  # lstm_1\n",
    "encoder_states = [state_h_enc, state_c_enc]\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_inputs = model.input[1]  # input_2\n",
    "decoder_state_input_h = Input(shape=(latent_dim,), name=\"input_3\")\n",
    "decoder_state_input_c = Input(shape=(latent_dim,), name=\"input_4\")\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_lstm = model.layers[3]\n",
    "decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs\n",
    ")\n",
    "decoder_states = [state_h_dec, state_c_dec]\n",
    "decoder_dense = model.layers[4]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",
    ")\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad94b0e",
   "metadata": {},
   "source": [
    "아래의 코드는 위의 추론 루프를 구현하는 데 사용했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ada07583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-9.4070899e-01, -2.7422613e-01,  4.4616241e-02, ...,\n",
       "          2.3093196e-02,  3.7427187e-01, -8.0107227e-02],\n",
       "        [-9.4070899e-01, -2.7422613e-01,  4.4616241e-02, ...,\n",
       "          2.3093196e-02,  3.7427187e-01, -8.0107227e-02],\n",
       "        [-9.4070899e-01, -2.7422613e-01,  4.4616241e-02, ...,\n",
       "          2.3093196e-02,  3.7427187e-01, -8.0107227e-02],\n",
       "        ...,\n",
       "        [-9.4526696e-01,  2.4986504e-02,  3.9154142e-02, ...,\n",
       "         -1.1905797e-03,  9.8464191e-01, -2.6607901e-01],\n",
       "        [-9.3928671e-01,  3.3071004e-02,  2.3990363e-02, ...,\n",
       "         -1.3091484e-03,  9.6853220e-01, -1.3747759e-01],\n",
       "        [-9.3572235e-01,  3.1161560e-02,  2.3867667e-02, ...,\n",
       "         -8.7526144e-04,  9.6929580e-01, -1.3895395e-01]], dtype=float32),\n",
       " array([[-4.006361  , -0.69437206,  3.8452244 , ...,  1.7800164 ,\n",
       "          1.1274245 , -2.9585857 ],\n",
       "        [-4.006361  , -0.69437206,  3.8452244 , ...,  1.7800164 ,\n",
       "          1.1274245 , -2.9585857 ],\n",
       "        [-4.006361  , -0.69437206,  3.8452244 , ...,  1.7800164 ,\n",
       "          1.1274245 , -2.9585857 ],\n",
       "        ...,\n",
       "        [-6.474272  ,  3.659115  ,  9.151735  , ..., -0.01816561,\n",
       "          4.156614  , -6.0615377 ],\n",
       "        [-3.4408035 ,  3.0375247 ,  9.071607  , ..., -0.03606099,\n",
       "          5.883499  , -7.1676836 ],\n",
       "        [-3.7902179 ,  3.186275  ,  8.947897  , ..., -0.02296546,\n",
       "          5.940004  , -7.3353963 ]], dtype=float32)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_model.predict(encoder_input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "34b1fcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 상태 벡터로서 입력값을 encode\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # 길이가 1인 빈 목표 시퀀스를 생성\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # 대상 시퀀스 첫 번째 문자를 시작 문자로 기재.\n",
    "    target_seq[0, 0, target_token_index[\"\\t\"]] = 1.0\n",
    "\n",
    "    # 시퀀스들의 batch에 대한 샘플링 반복(간소화를 위해, 배치 크기는 1로 상정)\n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # 토큰으로 샘플링\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # 탈출 조건 : 최대 길이에 도달하거나\n",
    "        # 종료 문자를 찾을 경우\n",
    "        if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n",
    "            stop_condition = True\n",
    "\n",
    "        # (길이 1인) 목표 시퀀스 최신화\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.0\n",
    "\n",
    "        # 상태 최신화\n",
    "        states_value = [h, c]\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc45a6f",
   "metadata": {},
   "source": [
    "몇 가지 좋은 결과를 얻게 됩니다. (학습 테스트에서 추출한 샘플을 해독하기에 놀랄만한 결과는 아니지만..)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c4b212",
   "metadata": {},
   "source": [
    "```bash\n",
    "Input sentence: Be nice.\n",
    "Decoded sentence: Soyez gentil !\n",
    "-\n",
    "Input sentence: Drop it!\n",
    "Decoded sentence: Laissez tomber !\n",
    "-\n",
    "Input sentence: Get out!\n",
    "Decoded sentence: Sortez !\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb90b41",
   "metadata": {},
   "source": [
    "이로써 keras의 Seq2Seq model에 대한 10분 안에 알려주기 튜토리얼을 마칩니다. \n",
    "알림 : 설명에 쓰인 예제 전체 code는 [Github](https://github.com/rstudio/keras/blob/master/vignettes/examples/lstm_seq2seq.py)에서 보실 수 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06957e23",
   "metadata": {},
   "source": [
    "### 참고문서\n",
    "* [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)\n",
    "* [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://arxiv.org/abs/1406.1078)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b8e3f2",
   "metadata": {},
   "source": [
    "# 추가 FAQ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d3f585",
   "metadata": {},
   "source": [
    "#### LSTM 대신 GRU 계층을 사용하려면 어떻게 해야 합니까?\n",
    "\n",
    "GRU는 오직 상태 1개만 가지지만 LSTM은 상태가 2개가 있기 때문에 실제론 약간 단순합니다.\n",
    "아래에 GRU 계층을 사용해 학습 모델을 조정하는 방법이 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9638a5c7",
   "metadata": {},
   "source": [
    "```python\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = GRU(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h = encoder(encoder_inputs)\n",
    "\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "decoder_gru = GRU(latent_dim, return_sequences=True)\n",
    "decoder_outputs = decoder_gru(decoder_inputs, initial_state=state_h)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924dde1f",
   "metadata": {},
   "source": [
    "#### 정수형 시퀀스가 포함된 단어단계 모델을 사용하려면 어떻게 해야 합니까?\n",
    "\n",
    "만약 입력이 정수형 시퀀스일 경우(예: 사전에서 색인으로 encode된 단어 시퀀스)라면?\n",
    "Embedding 계층을 통해서 정수형 토큰을 포함시킬 수 있습니다. 구현은 아래와 같습니다: \n",
    "\n",
    "```python\n",
    "# 입력 시퀀스 정의와 처리\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "x = Embedding(num_encoder_tokens, latent_dim)(encoder_inputs)\n",
    "x, state_h, state_c = LSTM(latent_dim,\n",
    "                           return_state=True)(x)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# `encoder_states`를 초기 상태로 사용해 decoder를 설정\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "x = Embedding(num_decoder_tokens, latent_dim)(decoder_inputs)\n",
    "x = LSTM(latent_dim, return_sequences=True)(x, initial_state=encoder_states)\n",
    "decoder_outputs = Dense(num_decoder_tokens, activation='softmax')(x)\n",
    "\n",
    "# `encoder_input_data`와 `decoder_input_data`를 `decoder_target_data`로 반환하도록 모델을 정의\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# 컴파일 & 학습 실행\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "# `decoder_target_data`는  `decoder_input_data` 같은 정수 시퀀스보단 one-hot 인코딩 형식이 되어야 함.\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d561dfc5",
   "metadata": {},
   "source": [
    "#### 학습하는 동안 Teacher forcing을 사용하지 않으려면 어떻게 해야 합니까?\n",
    "\n",
    "일부 환경에서 완전한 입력-목표 시퀀스 쌍을 버퍼링할 수 없듯이(예를 들어, 만약 매우 긴 시퀀스는 online 학습) 이는 전체 목표 시퀀스로 접근할 수 없기 때문에 Teacher forcing을 사용할 수 없습니다. 이 경우 decoder의 예측값을 입력으로 재입력하여 학습을 실행할 수 있습니다. (그저 추론될 수 있도록)\n",
    "\n",
    "출력값을 재주입하는 루프를 설계한 모델을 구축하면 다음과 같은 결과를 얻을 수 있습니다.\n",
    "\n",
    "```python\n",
    "from keras.layers import Lambda\n",
    "from keras import backend as K\n",
    "\n",
    "# 첫 부분은 바꿀 부분 없음.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "states = [state_h, state_c]\n",
    "\n",
    "\n",
    "# 한 번에 한 개의 time step이 진행되도록 decoder를 설정\n",
    "decoder_inputs = Input(shape=(1, num_decoder_tokens))\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "\n",
    "all_outputs = []\n",
    "inputs = decoder_inputs\n",
    "for _ in range(max_decoder_seq_length):\n",
    "    # 한 개의 time step에서 decoder 실행\n",
    "    outputs, state_h, state_c = decoder_lstm(inputs,\n",
    "                                             initial_state=states)\n",
    "    outputs = decoder_dense(outputs)\n",
    "    # 현재 예측치를 저장(나중에 모든 예측치를 연결할 수 있음)\n",
    "    all_outputs.append(outputs)\n",
    "    # 다음 반복을 위해 현 출력 데이터를 입력 데이터로 재지정하고 상태 또한 최신화.\n",
    "    inputs = outputs\n",
    "    states = [state_h, state_c]\n",
    "\n",
    "# 모든 예측치를 연결\n",
    "decoder_outputs = Lambda(lambda x: K.concatenate(x, axis=1))(all_outputs)\n",
    "\n",
    "# 앞에서처럼 모델을 정의하고 컴파일.\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "\n",
    "# 시작문자가 포함된 decoder 입력 정보\n",
    "decoder_input_data = np.zeros((num_samples, 1, num_decoder_tokens))\n",
    "decoder_input_data[:, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "# 앞에서처럼 모델 학습\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1130a8f9",
   "metadata": {},
   "source": [
    "만약 추가적인 의문점이 있다면, [Twitter](https://twitter.com/fchollet)로 연락해주세요.\n",
    "\n",
    "> 이 글은 2018 컨트리뷰톤에서 [`Contributue to Keras`](https://github.com/KerasKorea/KEKOxTutorial) 프로젝트로 진행했습니다.  \n",
    "> Translator : [mike2ox](https://github.com/mike2ox) (Moonhyeok Song)  \n",
    "> Translator Email : <firefinger07@gmail.com>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47110491",
   "metadata": {},
   "source": [
    "> 최신 예제는 https://keras.io/examples/nlp/lstm_seq2seq/ 에서 보실 수 있습니다.   \n",
    "> 본 ipynb는 코어닷투데이에서 활용하기 위해 구성되었습니다. http://github.com/coredottoday/DeepLearningTextBook/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
