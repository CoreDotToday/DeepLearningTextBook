{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "faceswap-GAN_lite_demo.ipynb의 사본",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "88EgdwFEenjn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2adef0b4-2ac0-47b2-e0d6-10f0766077a1"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Jul 13 08:23:12 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiOo3478pJg9"
      },
      "source": [
        "# Before we start...\n",
        "\n",
        "This colab notebook is a minimum demo for faceswap-GAN v2.2. Since colab allows maximum run time limit of 12 hrs, we will only train a lightweight model in this notebook. **The purpose of this notebook is not to train a model that produces high quality results but a quick overview for how faceswap-GAN works.**\n",
        "\n",
        "The pipeline of faceswap-GAN v2.2 is described below:\n",
        "\n",
        "  1. Upload two videos for training.\n",
        "  2. Apply face extraction (preprocessing) on the two uploaded videos\n",
        "  3. Train a liteweight faceswap-GAN model. (This will take 10 ~ 12 hrs)\n",
        "  4. Apply video conversion to the uploaded videos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "f7V1xkZIhrBv",
        "outputId": "1e80f6a4-3d35-4adc-d6cb-50abb261c973"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.5.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64hUDCEXh7OI",
        "outputId": "aa942359-9415-4e55-ef19-4ae0955db597"
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ozaE282k7I1"
      },
      "source": [
        "# pip install -U keras==2.3.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTHb9Oz7j_VP"
      },
      "source": [
        "# !pip install keras==2.1.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qf8Wvh5Wrm3V"
      },
      "source": [
        "# Step 1: Set runtime type to Python 3/GPU\n",
        "Set the colab notebook to GPU instance through: **runtime -> change runtime type -> Python3 and GPU**\n",
        "\n",
        "The following cells will show the system information of the current instance. Run the cells and check if it uses python >= 3.6 and has a GPU device."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zQhhPfKrmQl",
        "outputId": "89c59e53-ab2b-4f9f-dff1-de1b2a18f676"
      },
      "source": [
        "import platform\n",
        "print(platform.python_version())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3.7.10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1f96C1TApC00",
        "outputId": "760b262c-08b5-48a2-e61d-b937b94bf9b8"
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[name: \"/device:CPU:0\"\n",
              " device_type: \"CPU\"\n",
              " memory_limit: 268435456\n",
              " locality {\n",
              " }\n",
              " incarnation: 10136770639451533000, name: \"/device:XLA_CPU:0\"\n",
              " device_type: \"XLA_CPU\"\n",
              " memory_limit: 17179869184\n",
              " locality {\n",
              " }\n",
              " incarnation: 10358654129411381373\n",
              " physical_device_desc: \"device: XLA_CPU device\", name: \"/device:XLA_GPU:0\"\n",
              " device_type: \"XLA_GPU\"\n",
              " memory_limit: 17179869184\n",
              " locality {\n",
              " }\n",
              " incarnation: 2288141418612897404\n",
              " physical_device_desc: \"device: XLA_GPU device\", name: \"/device:GPU:0\"\n",
              " device_type: \"GPU\"\n",
              " memory_limit: 15964005991\n",
              " locality {\n",
              "   bus_id: 1\n",
              "   links {\n",
              "   }\n",
              " }\n",
              " incarnation: 17608277849682567298\n",
              " physical_device_desc: \"device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOsTN_gjzonZ"
      },
      "source": [
        "# Step 2: Git clone faceswap-GAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4VioT5YpJQB",
        "outputId": "88b291e9-ee45-475e-8add-520d410436cf"
      },
      "source": [
        "!git clone https://github.com/shaoanlu/faceswap-GAN.git"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'faceswap-GAN'...\n",
            "remote: Enumerating objects: 996, done.\u001b[K\n",
            "remote: Total 996 (delta 0), reused 0 (delta 0), pack-reused 996\u001b[K\n",
            "Receiving objects: 100% (996/996), 2.20 MiB | 3.88 MiB/s, done.\n",
            "Resolving deltas: 100% (609/609), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_4C8o3y0DFD",
        "outputId": "3abbd893-9f34-44d3-df0a-959ea31d4118"
      },
      "source": [
        "%cd \"faceswap-GAN\""
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/faceswap-GAN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0wsds0OslRc"
      },
      "source": [
        "# Step 3: Upload training videos\n",
        "\n",
        "The user should upload two videos: **source video** and **target video**. The model will **tranform source face to target face by default.**\n",
        "\n",
        "  - The videos better **contain only one person**.\n",
        "  - There is no limitation on video length but the longer it is, the longer preprocessing time / video conversion time it will take, which may cause excceded run time of 12 hrs. (**Recommended video length: 30 secs ~ 2 mins.**)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQ0HH6VUpJHW"
      },
      "source": [
        "from google.colab import files"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thlzp88qpJJk",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "outputId": "8a188753-3e0b-4c33-8317-8aa99609db06"
      },
      "source": [
        "# Upload source video\n",
        "source_video = files.upload()\n",
        "\n",
        "for fn_source_video, _ in source_video.items():\n",
        "    print(fn_source_video)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a8386124-a092-45d1-b826-04a04cf11356\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a8386124-a092-45d1-b826-04a04cf11356\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyKu2hcCpJKu"
      },
      "source": [
        "# Upload target video\n",
        "target_video = files.upload()\n",
        "\n",
        "for fn_target_video, _ in target_video.items():\n",
        "    print(fn_target_video)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xkAen9dduOQ"
      },
      "source": [
        "# Step 4: Set maximum training iterations\n",
        "Default 25000 iters require ~ 10hrs of training.\n",
        "\n",
        "Iterations >= 27k may exceed run time limit; Iterations < 18k may yield poorly-trained model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51-p9Fm_duOR"
      },
      "source": [
        "global TOTAL_ITERS\n",
        "TOTAL_ITERS = 34000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PS25Uu9kxDwo"
      },
      "source": [
        "# Step 5: Everything is ready.\n",
        "\n",
        "**Press Ctrl + F10 (or runtime -> run after)** to start the remaining process and leave this page alone. It will take 10 ~ 12 hours to finish training. The result video can be downloaded by running the last cell: \n",
        "  ```python\n",
        "  files.download(\"OUTPUT_VIDEO.mp4\")\n",
        "  # Some browsers do not support this line (e.g., Opera does not pop up a save dialog). Please use Firefox or Chrome.\n",
        "  ```\n",
        "Notice that **this page should not be closed or refreshed while running**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BY3qysVq0p2P"
      },
      "source": [
        "%%capture\n",
        "!pip install moviepy\n",
        "!pip install keras_vggface\n",
        "import imageio\n",
        "imageio.plugins.ffmpeg.download()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ym0EsJk9pJRw"
      },
      "source": [
        "import keras.backend as K\n",
        "from detector.face_detector import MTCNNFaceDetector\n",
        "import glob\n",
        "\n",
        "from preprocess import preprocess_video"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otuzS3Di0gvz"
      },
      "source": [
        "fd = MTCNNFaceDetector(sess=K.get_session(), model_path=\"./mtcnn_weights/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kt5FVEt11B2K"
      },
      "source": [
        "!mkdir -p faceA/rgb\n",
        "!mkdir -p faceA/binary_mask\n",
        "!mkdir -p faceB/rgb\n",
        "!mkdir -p faceB/binary_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dO11aRsZ0gyK"
      },
      "source": [
        "save_interval = 5 # perform face detection every {save_interval} frames\n",
        "save_path = \"./faceA/\"\n",
        "preprocess_video(fn_source_video, fd, save_interval, save_path)\n",
        "save_path = \"./faceB/\"\n",
        "preprocess_video(fn_target_video, fd, save_interval, save_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIb9TSMz0g0g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "outputId": "43087792-b09c-4491-c334-3b4ec7aaa734"
      },
      "source": [
        "print(str(len(glob.glob(\"faceA/rgb/*.*\"))) + \" face(s) extracted from source video: \" + fn_source_video + \".\")\n",
        "print(str(len(glob.glob(\"faceB/rgb/*.*\"))) + \" face(s) extracted from target video: \" + fn_target_video + \".\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-a696fd2ffa48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"faceA/rgb/*.*\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" face(s) extracted from source video: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfn_source_video\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"faceB/rgb/*.*\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" face(s) extracted from target video: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfn_target_video\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'fn_source_video' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9L5UW8U3AaQ"
      },
      "source": [
        "## The following cells are from [FaceSwap_GAN_v2.2_train_test.ipynb](https://github.com/shaoanlu/faceswap-GAN/blob/master/FaceSwap_GAN_v2.2_train_test.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtyeXpEc2lDO"
      },
      "source": [
        "## Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtENXluJ0g2I"
      },
      "source": [
        "from keras.layers import *\n",
        "import keras.backend as K\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFAokeU12Vff"
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "import time\n",
        "import numpy as np\n",
        "from pathlib import PurePath, Path\n",
        "from IPython.display import clear_output\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJ1miHVk2ns7"
      },
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjrPsIbZ2ViU"
      },
      "source": [
        "K.set_learning_phase(1)\n",
        "# Number of CPU cores\n",
        "num_cpus = os.cpu_count()\n",
        "\n",
        "# Input/Output resolution\n",
        "RESOLUTION = 64 # 64x64, 128x128, 256x256\n",
        "assert (RESOLUTION % 64) == 0, \"RESOLUTION should be 64, 128, or 256.\"\n",
        "\n",
        "# Batch size\n",
        "batchSize = 4\n",
        "\n",
        "# Use motion blurs (data augmentation)\n",
        "# set True if training data contains images extracted from videos\n",
        "use_da_motion_blur = False \n",
        "\n",
        "# Use eye-aware training\n",
        "# require images generated from prep_binary_masks.ipynb\n",
        "use_bm_eyes = True\n",
        "\n",
        "# Probability of random color matching (data augmentation)\n",
        "prob_random_color_match = 0.5\n",
        "\n",
        "da_config = {\n",
        "    \"prob_random_color_match\": prob_random_color_match,\n",
        "    \"use_da_motion_blur\": use_da_motion_blur,\n",
        "    \"use_bm_eyes\": use_bm_eyes\n",
        "}\n",
        "\n",
        "# Path to training images\n",
        "img_dirA = './faceA/rgb'\n",
        "img_dirB = './faceB/rgb'\n",
        "img_dirA_bm_eyes = \"./faceA/binary_mask\"\n",
        "img_dirB_bm_eyes = \"./faceB/binary_mask\"\n",
        "\n",
        "# Path to saved model weights\n",
        "models_dir = \"./models\"\n",
        "\n",
        "# Architecture configuration\n",
        "arch_config = {}\n",
        "arch_config['IMAGE_SHAPE'] = (RESOLUTION, RESOLUTION, 3)\n",
        "arch_config['use_self_attn'] = True\n",
        "arch_config['norm'] = \"hybrid\" # instancenorm, batchnorm, layernorm, groupnorm, none\n",
        "arch_config['model_capacity'] = \"lite\" # standard, lite\n",
        "\n",
        "# Loss function weights configuration\n",
        "loss_weights = {}\n",
        "loss_weights['w_D'] = 0.1 # Discriminator\n",
        "loss_weights['w_recon'] = 1. # L1 reconstruction loss\n",
        "loss_weights['w_edge'] = 0.1 # edge loss\n",
        "loss_weights['w_eyes'] = 30. # reconstruction and edge loss on eyes area\n",
        "loss_weights['w_pl'] = (0.01, 0.1, 0.3, 0.1) # perceptual loss (0.003, 0.03, 0.3, 0.3)\n",
        "\n",
        "# Init. loss config.\n",
        "loss_config = {}\n",
        "loss_config[\"gan_training\"] = \"mixup_LSGAN\"\n",
        "loss_config['use_PL'] = False\n",
        "loss_config[\"PL_before_activ\"] = True\n",
        "loss_config['use_mask_hinge_loss'] = False\n",
        "loss_config['m_mask'] = 0.\n",
        "loss_config['lr_factor'] = 1.\n",
        "loss_config['use_cyclic_loss'] = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSrt2h0K3so3"
      },
      "source": [
        "## Build the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5o2dEVW92Vms"
      },
      "source": [
        "from networks.faceswap_gan_model import FaceswapGANModel\n",
        "from data_loader.data_loader import DataLoader\n",
        "from utils import showG, showG_mask, showG_eyes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMQxeH7EjsCy"
      },
      "source": [
        "Keras 버전 문제로 에러가 날 수 있습니다. (`NameError: name 'regularizers' is not defined`)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiNsc3N_2VhU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8830b70c-55e0-406f-abee-7bd4b0e3f640"
      },
      "source": [
        "model = FaceswapGANModel(**arch_config)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "tracking <tf.Variable 'scale_1/scale_1_gamma:0' shape=(1,) dtype=float32> gamma\n",
            "tracking <tf.Variable 'scale_2/scale_2_gamma:0' shape=(1,) dtype=float32> gamma\n",
            "tracking <tf.Variable 'scale_3/scale_3_gamma:0' shape=(1,) dtype=float32> gamma\n",
            "tracking <tf.Variable 'scale_4/scale_4_gamma:0' shape=(1,) dtype=float32> gamma\n",
            "tracking <tf.Variable 'scale_5/scale_5_gamma:0' shape=(1,) dtype=float32> gamma\n",
            "tracking <tf.Variable 'scale_6/scale_6_gamma:0' shape=(1,) dtype=float32> gamma\n",
            "tracking <tf.Variable 'scale_7/scale_7_gamma:0' shape=(1,) dtype=float32> gamma\n",
            "tracking <tf.Variable 'scale_8/scale_8_gamma:0' shape=(1,) dtype=float32> gamma\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4DfL5kJduOY"
      },
      "source": [
        "%%capture\n",
        "!wget https://github.com/rcmalli/keras-vggface/releases/download/v2.0/rcmalli_vggface_tf_notop_resnet50.h5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvVr2TqI3jd9",
        "outputId": "31d31b23-6f2a-4a13-9923-7f89f22f4a25"
      },
      "source": [
        "#from keras_vggface.vggface import VGGFace\n",
        "\n",
        "# VGGFace ResNet50\n",
        "#vggface = VGGFace(include_top=False, model='resnet50', input_shape=(224, 224, 3))'\n",
        "\n",
        "from colab_demo.vggface_models import RESNET50\n",
        "vggface = RESNET50(include_top=False, weights=None, input_shape=(224, 224, 3))\n",
        "vggface.load_weights(\"rcmalli_vggface_tf_notop_resnet50.h5\")\n",
        "\n",
        "#from keras.applications.resnet50 import ResNet50\n",
        "#vggface = ResNet50(include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "#vggface.summary()\n",
        "\n",
        "model.build_pl_model(vggface_model=vggface, before_activ=loss_config[\"PL_before_activ\"])\n",
        "model.build_train_functions(loss_weights=loss_weights, **loss_config)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/keras/backend/tensorflow_backend.py:4074: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/faceswap-GAN/networks/losses.py:44: Beta.__init__ (from tensorflow.python.ops.distributions.beta) is deprecated and will be removed after 2019-01-01.\n",
            "Instructions for updating:\n",
            "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/distributions/beta.py:208: Distribution.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
            "Instructions for updating:\n",
            "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
            "WARNING:tensorflow:From /content/faceswap-GAN/networks/losses.py:94: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GexJ-i7c3vz2"
      },
      "source": [
        "## Start training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoCnWQiR3jgd"
      },
      "source": [
        "# Create ./models directory\n",
        "Path(f\"models\").mkdir(parents=True, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjkSygNT3jjB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0e7303b-906e-4b73-fb54-051a5574d92e"
      },
      "source": [
        "# Get filenames\n",
        "train_A = glob.glob(img_dirA+\"/*.*\")\n",
        "train_B = glob.glob(img_dirB+\"/*.*\")\n",
        "\n",
        "train_AnB = train_A + train_B\n",
        "\n",
        "assert len(train_A), \"No image found in \" + str(img_dirA)\n",
        "assert len(train_B), \"No image found in \" + str(img_dirB)\n",
        "print (\"Number of images in folder A: \" + str(len(train_A)))\n",
        "print (\"Number of images in folder B: \" + str(len(train_B)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of images in folder A: 6\n",
            "Number of images in folder B: 9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KzKahMU3jlX"
      },
      "source": [
        "def show_loss_config(loss_config):\n",
        "    for config, value in loss_config.items():\n",
        "        print(f\"{config} = {value}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5fwrKth3jqO"
      },
      "source": [
        "def reset_session(save_path):\n",
        "    global model, vggface\n",
        "    global train_batchA, train_batchB\n",
        "    model.save_weights(path=save_path)\n",
        "    del model\n",
        "    del vggface\n",
        "    del train_batchA\n",
        "    del train_batchB\n",
        "    K.clear_session()\n",
        "    model = FaceswapGANModel(**arch_config)\n",
        "    model.load_weights(path=save_path)\n",
        "    #vggface = VGGFace(include_top=False, model='resnet50', input_shape=(224, 224, 3))\n",
        "    vggface = RESNET50(include_top=False, weights=None, input_shape=(224, 224, 3))\n",
        "    vggface.load_weights(\"rcmalli_vggface_tf_notop_resnet50.h5\")\n",
        "    model.build_pl_model(vggface_model=vggface, before_activ=loss_config[\"PL_before_activ\"])\n",
        "    train_batchA = DataLoader(train_A, train_AnB, batchSize, img_dirA_bm_eyes,\n",
        "                              RESOLUTION, num_cpus, K.get_session(), **da_config)\n",
        "    train_batchB = DataLoader(train_B, train_AnB, batchSize, img_dirB_bm_eyes, \n",
        "                              RESOLUTION, num_cpus, K.get_session(), **da_config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ll7g7mGU3jss",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 851
        },
        "outputId": "8bb8d687-d0e9-4524-e0a4-cfcdd2eb04e1"
      },
      "source": [
        "# Start training\n",
        "t0 = time.time()\n",
        "\n",
        "# This try/except is meant to resume training if we disconnected from Colab\n",
        "try:\n",
        "    gen_iterations\n",
        "    print(f\"Resume training from iter {gen_iterations}.\")\n",
        "except:\n",
        "    gen_iterations = 0\n",
        "\n",
        "errGA_sum = errGB_sum = errDA_sum = errDB_sum = 0\n",
        "errGAs = {}\n",
        "errGBs = {}\n",
        "# Dictionaries are ordered in Python 3.6\n",
        "for k in ['ttl', 'adv', 'recon', 'edge', 'pl']:\n",
        "    errGAs[k] = 0\n",
        "    errGBs[k] = 0\n",
        "\n",
        "display_iters = 300\n",
        "global TOTAL_ITERS\n",
        "\n",
        "global train_batchA, train_batchB\n",
        "train_batchA = DataLoader(train_A, train_AnB, batchSize, img_dirA_bm_eyes, \n",
        "                          RESOLUTION, num_cpus, K.get_session(), **da_config)\n",
        "train_batchB = DataLoader(train_B, train_AnB, batchSize, img_dirB_bm_eyes, \n",
        "                          RESOLUTION, num_cpus, K.get_session(), **da_config)\n",
        "\n",
        "while gen_iterations <= TOTAL_ITERS: \n",
        "    \n",
        "    # Loss function automation\n",
        "    if gen_iterations == (TOTAL_ITERS//5 - display_iters//2):\n",
        "        clear_output()\n",
        "        loss_config['use_PL'] = True\n",
        "        loss_config['use_mask_hinge_loss'] = False\n",
        "        loss_config['m_mask'] = 0.0\n",
        "        reset_session(models_dir)\n",
        "        print(\"Building new loss funcitons...\")\n",
        "        show_loss_config(loss_config)\n",
        "        model.build_train_functions(loss_weights=loss_weights, **loss_config)\n",
        "        print(\"Done.\")\n",
        "    elif gen_iterations == (TOTAL_ITERS//5 + TOTAL_ITERS//10 - display_iters//2):\n",
        "        clear_output()\n",
        "        loss_config['use_PL'] = True\n",
        "        loss_config['use_mask_hinge_loss'] = True\n",
        "        loss_config['m_mask'] = 0.5\n",
        "        reset_session(models_dir)\n",
        "        print(\"Building new loss funcitons...\")\n",
        "        show_loss_config(loss_config)\n",
        "        model.build_train_functions(loss_weights=loss_weights, **loss_config)\n",
        "        print(\"Complete.\")\n",
        "    elif gen_iterations == (2*TOTAL_ITERS//5 - display_iters//2):\n",
        "        clear_output()\n",
        "        loss_config['use_PL'] = True\n",
        "        loss_config['use_mask_hinge_loss'] = True\n",
        "        loss_config['m_mask'] = 0.2\n",
        "        reset_session(models_dir)\n",
        "        print(\"Building new loss funcitons...\")\n",
        "        show_loss_config(loss_config)\n",
        "        model.build_train_functions(loss_weights=loss_weights, **loss_config)\n",
        "        print(\"Done.\")\n",
        "    elif gen_iterations == (TOTAL_ITERS//2 - display_iters//2):\n",
        "        clear_output()\n",
        "        loss_config['use_PL'] = True\n",
        "        loss_config['use_mask_hinge_loss'] = True\n",
        "        loss_config['m_mask'] = 0.4\n",
        "        loss_config['lr_factor'] = 0.3\n",
        "        reset_session(models_dir)\n",
        "        print(\"Building new loss funcitons...\")\n",
        "        show_loss_config(loss_config)\n",
        "        model.build_train_functions(loss_weights=loss_weights, **loss_config)\n",
        "        print(\"Done.\")\n",
        "    elif gen_iterations == (2*TOTAL_ITERS//3 - display_iters//2):\n",
        "        clear_output()\n",
        "        model.decoder_A.load_weights(\"models/decoder_B.h5\") # swap decoders\n",
        "        model.decoder_B.load_weights(\"models/decoder_A.h5\") # swap decoders\n",
        "        loss_config['use_PL'] = True\n",
        "        loss_config['use_mask_hinge_loss'] = True\n",
        "        loss_config['m_mask'] = 0.5\n",
        "        loss_config['lr_factor'] = 1\n",
        "        reset_session(models_dir)\n",
        "        print(\"Building new loss funcitons...\")\n",
        "        show_loss_config(loss_config)\n",
        "        model.build_train_functions(loss_weights=loss_weights, **loss_config)\n",
        "        print(\"Done.\")\n",
        "    elif gen_iterations == (8*TOTAL_ITERS//10 - display_iters//2):\n",
        "        clear_output()\n",
        "        loss_config['use_PL'] = True\n",
        "        loss_config['use_mask_hinge_loss'] = True\n",
        "        loss_config['m_mask'] = 0.1\n",
        "        loss_config['lr_factor'] = 0.3\n",
        "        reset_session(models_dir)\n",
        "        print(\"Building new loss funcitons...\")\n",
        "        show_loss_config(loss_config)\n",
        "        model.build_train_functions(loss_weights=loss_weights, **loss_config)\n",
        "        print(\"Done.\")\n",
        "    elif gen_iterations == (9*TOTAL_ITERS//10 - display_iters//2):\n",
        "        clear_output()\n",
        "        loss_config['use_PL'] = True\n",
        "        loss_config['use_mask_hinge_loss'] = False\n",
        "        loss_config['m_mask'] = 0.0\n",
        "        loss_config['lr_factor'] = 0.1\n",
        "        reset_session(models_dir)\n",
        "        print(\"Building new loss funcitons...\")\n",
        "        show_loss_config(loss_config)\n",
        "        model.build_train_functions(loss_weights=loss_weights, **loss_config)\n",
        "        print(\"Done.\")\n",
        "    \n",
        "    if gen_iterations == 5:\n",
        "        print (\"working.\")\n",
        "    \n",
        "    # Train dicriminators for one batch\n",
        "    data_A = train_batchA.get_next_batch()\n",
        "    data_B = train_batchB.get_next_batch()\n",
        "    errDA, errDB = model.train_one_batch_D(data_A=data_A, data_B=data_B)\n",
        "    errDA_sum +=errDA[0]\n",
        "    errDB_sum +=errDB[0]\n",
        "\n",
        "    # Train generators for one batch\n",
        "    data_A = train_batchA.get_next_batch()\n",
        "    data_B = train_batchB.get_next_batch()\n",
        "    errGA, errGB = model.train_one_batch_G(data_A=data_A, data_B=data_B)\n",
        "    errGA_sum += errGA[0]\n",
        "    errGB_sum += errGB[0]\n",
        "    for i, k in enumerate(['ttl', 'adv', 'recon', 'edge', 'pl']):\n",
        "        errGAs[k] += errGA[i]\n",
        "        errGBs[k] += errGB[i]\n",
        "    gen_iterations+=1\n",
        "    \n",
        "    # Visualization\n",
        "    if gen_iterations % display_iters == 0:\n",
        "        clear_output()\n",
        "            \n",
        "        # Display loss information\n",
        "        show_loss_config(loss_config)\n",
        "        print(\"----------\") \n",
        "        print('[iter %d] Loss_DA: %f Loss_DB: %f Loss_GA: %f Loss_GB: %f time: %f'\n",
        "        % (gen_iterations, errDA_sum/display_iters, errDB_sum/display_iters,\n",
        "           errGA_sum/display_iters, errGB_sum/display_iters, time.time()-t0))  \n",
        "        print(\"----------\") \n",
        "        print(\"Generator loss details:\")\n",
        "        print(f'[Adversarial loss]')  \n",
        "        print(f'GA: {errGAs[\"adv\"]/display_iters:.4f} GB: {errGBs[\"adv\"]/display_iters:.4f}')\n",
        "        print(f'[Reconstruction loss]')\n",
        "        print(f'GA: {errGAs[\"recon\"]/display_iters:.4f} GB: {errGBs[\"recon\"]/display_iters:.4f}')\n",
        "        print(f'[Edge loss]')\n",
        "        print(f'GA: {errGAs[\"edge\"]/display_iters:.4f} GB: {errGBs[\"edge\"]/display_iters:.4f}')\n",
        "        if loss_config['use_PL'] == True:\n",
        "            print(f'[Perceptual loss]')\n",
        "            try:\n",
        "                print(f'GA: {errGAs[\"pl\"][0]/display_iters:.4f} GB: {errGBs[\"pl\"][0]/display_iters:.4f}')\n",
        "            except:\n",
        "                print(f'GA: {errGAs[\"pl\"]/display_iters:.4f} GB: {errGBs[\"pl\"]/display_iters:.4f}')\n",
        "        \n",
        "        # Display images\n",
        "        print(\"----------\") \n",
        "        wA, tA, _ = train_batchA.get_next_batch()\n",
        "        wB, tB, _ = train_batchB.get_next_batch()\n",
        "        print(\"Transformed (masked) results:\")\n",
        "        showG(tA, tB, model.path_A, model.path_B, batchSize)   \n",
        "        print(\"Masks:\")\n",
        "        showG_mask(tA, tB, model.path_mask_A, model.path_mask_B, batchSize)  \n",
        "        print(\"Reconstruction results:\")\n",
        "        showG(wA, wB, model.path_bgr_A, model.path_bgr_B, batchSize)           \n",
        "        errGA_sum = errGB_sum = errDA_sum = errDB_sum = 0\n",
        "        for k in ['ttl', 'adv', 'recon', 'edge', 'pl']:\n",
        "            errGAs[k] = 0\n",
        "            errGBs[k] = 0\n",
        "        \n",
        "        # Save models\n",
        "        model.save_weights(path=models_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-ca3e1382b1d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mgen_iterations\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mTOTAL_ITERS\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m3\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdisplay_iters\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder_A\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"models/decoder_B.h5\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# swap decoders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder_B\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"models/decoder_A.h5\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# swap decoders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mloss_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'use_PL'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mload_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/keras/engine/network.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, reshape)\u001b[0m\n\u001b[1;32m   1228\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1229\u001b[0m                 saving.load_weights_from_hdf5_group(\n\u001b[0;32m-> 1230\u001b[0;31m                     f, self.layers, reshape=reshape)\n\u001b[0m\u001b[1;32m   1231\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'close'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1232\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, layers, reshape)\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mfiltered_layers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1197\u001b[0;31m     \u001b[0mlayer_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_attributes_from_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'layer_names'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1198\u001b[0m     \u001b[0mfiltered_layer_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayer_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_attributes_from_hdf5_group\u001b[0;34m(group, name)\u001b[0m\n\u001b[1;32m    720\u001b[0m     \"\"\"\n\u001b[1;32m    721\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/keras/engine/saving.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    720\u001b[0m     \"\"\"\n\u001b[1;32m    721\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'decode'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "BHMkwy9eXtBX",
        "outputId": "f8abd15f-0ed6-4e77-ad70-e0b847ada86c"
      },
      "source": [
        "model.decoder_A.load_weights(\"models/decoder_B.h5\") # swap decoders\n",
        "model.decoder_B.load_weights(\"models/decoder_A.h5\") # swap decoders\n",
        "loss_config['use_PL'] = True\n",
        "loss_config['use_mask_hinge_loss'] = True\n",
        "loss_config['m_mask'] = 0.5\n",
        "loss_config['lr_factor'] = 1\n",
        "reset_session(models_dir)\n",
        "print(\"Building new loss funcitons...\")\n",
        "show_loss_config(loss_config)\n",
        "model.build_train_functions(loss_weights=loss_weights, **loss_config)\n",
        "print(\"Done.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model weights files have been saved to ./models.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-9f7310b4f6f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mloss_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'm_mask'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mloss_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr_factor'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mreset_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Building new loss funcitons...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mshow_loss_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-24d24b103788>\u001b[0m in \u001b[0;36mreset_session\u001b[0;34m(save_path)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mdel\u001b[0m \u001b[0mvggface\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mtrain_batchA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mtrain_batchB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'vggface' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omuwNdx8Xnho"
      },
      "source": [
        "# Start training\n",
        "t0 = time.time()\n",
        "\n",
        "# This try/except is meant to resume training if we disconnected from Colab\n",
        "try:\n",
        "    gen_iterations\n",
        "    print(f\"Resume training from iter {gen_iterations}.\")\n",
        "except:\n",
        "    gen_iterations = 0\n",
        "\n",
        "errGA_sum = errGB_sum = errDA_sum = errDB_sum = 0\n",
        "errGAs = {}\n",
        "errGBs = {}\n",
        "# Dictionaries are ordered in Python 3.6\n",
        "for k in ['ttl', 'adv', 'recon', 'edge', 'pl']:\n",
        "    errGAs[k] = 0\n",
        "    errGBs[k] = 0\n",
        "\n",
        "display_iters = 300\n",
        "global TOTAL_ITERS\n",
        "\n",
        "global train_batchA, train_batchB\n",
        "train_batchA = DataLoader(train_A, train_AnB, batchSize, img_dirA_bm_eyes, \n",
        "                          RESOLUTION, num_cpus, K.get_session(), **da_config)\n",
        "train_batchB = DataLoader(train_B, train_AnB, batchSize, img_dirB_bm_eyes, \n",
        "                          RESOLUTION, num_cpus, K.get_session(), **da_config)\n",
        "\n",
        "while gen_iterations <= TOTAL_ITERS: \n",
        "    \n",
        "    # Loss function automation\n",
        "    if gen_iterations == (TOTAL_ITERS//5 - display_iters//2):\n",
        "        clear_output()\n",
        "        loss_config['use_PL'] = True\n",
        "        loss_config['use_mask_hinge_loss'] = False\n",
        "        loss_config['m_mask'] = 0.0\n",
        "        reset_session(models_dir)\n",
        "        print(\"Building new loss funcitons...\")\n",
        "        show_loss_config(loss_config)\n",
        "        model.build_train_functions(loss_weights=loss_weights, **loss_config)\n",
        "        print(\"Done.\")\n",
        "    elif gen_iterations == (TOTAL_ITERS//5 + TOTAL_ITERS//10 - display_iters//2):\n",
        "        clear_output()\n",
        "        loss_config['use_PL'] = True\n",
        "        loss_config['use_mask_hinge_loss'] = True\n",
        "        loss_config['m_mask'] = 0.5\n",
        "        reset_session(models_dir)\n",
        "        print(\"Building new loss funcitons...\")\n",
        "        show_loss_config(loss_config)\n",
        "        model.build_train_functions(loss_weights=loss_weights, **loss_config)\n",
        "        print(\"Complete.\")\n",
        "    elif gen_iterations == (2*TOTAL_ITERS//5 - display_iters//2):\n",
        "        clear_output()\n",
        "        loss_config['use_PL'] = True\n",
        "        loss_config['use_mask_hinge_loss'] = True\n",
        "        loss_config['m_mask'] = 0.2\n",
        "        reset_session(models_dir)\n",
        "        print(\"Building new loss funcitons...\")\n",
        "        show_loss_config(loss_config)\n",
        "        model.build_train_functions(loss_weights=loss_weights, **loss_config)\n",
        "        print(\"Done.\")\n",
        "    elif gen_iterations == (TOTAL_ITERS//2 - display_iters//2):\n",
        "        clear_output()\n",
        "        loss_config['use_PL'] = True\n",
        "        loss_config['use_mask_hinge_loss'] = True\n",
        "        loss_config['m_mask'] = 0.4\n",
        "        loss_config['lr_factor'] = 0.3\n",
        "        reset_session(models_dir)\n",
        "        print(\"Building new loss funcitons...\")\n",
        "        show_loss_config(loss_config)\n",
        "        model.build_train_functions(loss_weights=loss_weights, **loss_config)\n",
        "        print(\"Done.\")\n",
        "    elif gen_iterations == (2*TOTAL_ITERS//3 - display_iters//2):\n",
        "        clear_output()\n",
        "        model.decoder_A.load_weights(\"models/decoder_B.h5\") # swap decoders\n",
        "        model.decoder_B.load_weights(\"models/decoder_A.h5\") # swap decoders\n",
        "        loss_config['use_PL'] = True\n",
        "        loss_config['use_mask_hinge_loss'] = True\n",
        "        loss_config['m_mask'] = 0.5\n",
        "        loss_config['lr_factor'] = 1\n",
        "        reset_session(models_dir)\n",
        "        print(\"Building new loss funcitons...\")\n",
        "        show_loss_config(loss_config)\n",
        "        model.build_train_functions(loss_weights=loss_weights, **loss_config)\n",
        "        print(\"Done.\")\n",
        "    elif gen_iterations == (8*TOTAL_ITERS//10 - display_iters//2):\n",
        "        clear_output()\n",
        "        loss_config['use_PL'] = True\n",
        "        loss_config['use_mask_hinge_loss'] = True\n",
        "        loss_config['m_mask'] = 0.1\n",
        "        loss_config['lr_factor'] = 0.3\n",
        "        reset_session(models_dir)\n",
        "        print(\"Building new loss funcitons...\")\n",
        "        show_loss_config(loss_config)\n",
        "        model.build_train_functions(loss_weights=loss_weights, **loss_config)\n",
        "        print(\"Done.\")\n",
        "    elif gen_iterations == (9*TOTAL_ITERS//10 - display_iters//2):\n",
        "        clear_output()\n",
        "        loss_config['use_PL'] = True\n",
        "        loss_config['use_mask_hinge_loss'] = False\n",
        "        loss_config['m_mask'] = 0.0\n",
        "        loss_config['lr_factor'] = 0.1\n",
        "        reset_session(models_dir)\n",
        "        print(\"Building new loss funcitons...\")\n",
        "        show_loss_config(loss_config)\n",
        "        model.build_train_functions(loss_weights=loss_weights, **loss_config)\n",
        "        print(\"Done.\")\n",
        "    \n",
        "    if gen_iterations == 5:\n",
        "        print (\"working.\")\n",
        "    \n",
        "    # Train dicriminators for one batch\n",
        "    data_A = train_batchA.get_next_batch()\n",
        "    data_B = train_batchB.get_next_batch()\n",
        "    errDA, errDB = model.train_one_batch_D(data_A=data_A, data_B=data_B)\n",
        "    errDA_sum +=errDA[0]\n",
        "    errDB_sum +=errDB[0]\n",
        "\n",
        "    # Train generators for one batch\n",
        "    data_A = train_batchA.get_next_batch()\n",
        "    data_B = train_batchB.get_next_batch()\n",
        "    errGA, errGB = model.train_one_batch_G(data_A=data_A, data_B=data_B)\n",
        "    errGA_sum += errGA[0]\n",
        "    errGB_sum += errGB[0]\n",
        "    for i, k in enumerate(['ttl', 'adv', 'recon', 'edge', 'pl']):\n",
        "        errGAs[k] += errGA[i]\n",
        "        errGBs[k] += errGB[i]\n",
        "    gen_iterations+=1\n",
        "    \n",
        "    # Visualization\n",
        "    if gen_iterations % display_iters == 0:\n",
        "        clear_output()\n",
        "            \n",
        "        # Display loss information\n",
        "        show_loss_config(loss_config)\n",
        "        print(\"----------\") \n",
        "        print('[iter %d] Loss_DA: %f Loss_DB: %f Loss_GA: %f Loss_GB: %f time: %f'\n",
        "        % (gen_iterations, errDA_sum/display_iters, errDB_sum/display_iters,\n",
        "           errGA_sum/display_iters, errGB_sum/display_iters, time.time()-t0))  \n",
        "        print(\"----------\") \n",
        "        print(\"Generator loss details:\")\n",
        "        print(f'[Adversarial loss]')  \n",
        "        print(f'GA: {errGAs[\"adv\"]/display_iters:.4f} GB: {errGBs[\"adv\"]/display_iters:.4f}')\n",
        "        print(f'[Reconstruction loss]')\n",
        "        print(f'GA: {errGAs[\"recon\"]/display_iters:.4f} GB: {errGBs[\"recon\"]/display_iters:.4f}')\n",
        "        print(f'[Edge loss]')\n",
        "        print(f'GA: {errGAs[\"edge\"]/display_iters:.4f} GB: {errGBs[\"edge\"]/display_iters:.4f}')\n",
        "        if loss_config['use_PL'] == True:\n",
        "            print(f'[Perceptual loss]')\n",
        "            try:\n",
        "                print(f'GA: {errGAs[\"pl\"][0]/display_iters:.4f} GB: {errGBs[\"pl\"][0]/display_iters:.4f}')\n",
        "            except:\n",
        "                print(f'GA: {errGAs[\"pl\"]/display_iters:.4f} GB: {errGBs[\"pl\"]/display_iters:.4f}')\n",
        "        \n",
        "        # Display images\n",
        "        print(\"----------\") \n",
        "        wA, tA, _ = train_batchA.get_next_batch()\n",
        "        wB, tB, _ = train_batchB.get_next_batch()\n",
        "        print(\"Transformed (masked) results:\")\n",
        "        showG(tA, tB, model.path_A, model.path_B, batchSize)   \n",
        "        print(\"Masks:\")\n",
        "        showG_mask(tA, tB, model.path_mask_A, model.path_mask_B, batchSize)  \n",
        "        print(\"Reconstruction results:\")\n",
        "        showG(wA, wB, model.path_bgr_A, model.path_bgr_B, batchSize)           \n",
        "        errGA_sum = errGB_sum = errDA_sum = errDB_sum = 0\n",
        "        for k in ['ttl', 'adv', 'recon', 'edge', 'pl']:\n",
        "            errGAs[k] = 0\n",
        "            errGBs[k] = 0\n",
        "        \n",
        "        # Save models\n",
        "        model.save_weights(path=models_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRD_02DL3ju9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7d4b2df-ea7f-46f3-cc3a-a886d2b7fef3"
      },
      "source": [
        "1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSqr_UTX4wRE"
      },
      "source": [
        "## The following cells are from [FaceSwap_GAN_v2.2_video_conversion.ipynb](https://github.com/shaoanlu/faceswap-GAN/blob/master/FaceSwap_GAN_v2.2_video_conversion.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x58lrjkw6iQM"
      },
      "source": [
        "## Video conversion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOi2ZsSa4vf4"
      },
      "source": [
        "from converter.video_converter import VideoConverter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONnQFokfduOh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "634b323d-ad79-4162-8f32-e22c5e44c52f"
      },
      "source": [
        "global model, vggface\n",
        "global train_batchA, train_batchB\n",
        "del model\n",
        "del vggface\n",
        "del train_batchA\n",
        "del train_batchB\n",
        "tf.reset_default_graph()\n",
        "K.clear_session()\n",
        "model = FaceswapGANModel(**arch_config)\n",
        "model.load_weights(path=models_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tracking <tf.Variable 'scale_1/scale_1_gamma:0' shape=(1,) dtype=float32> gamma\n",
            "tracking <tf.Variable 'scale_2/scale_2_gamma:0' shape=(1,) dtype=float32> gamma\n",
            "tracking <tf.Variable 'scale_3/scale_3_gamma:0' shape=(1,) dtype=float32> gamma\n",
            "tracking <tf.Variable 'scale_4/scale_4_gamma:0' shape=(1,) dtype=float32> gamma\n",
            "tracking <tf.Variable 'scale_5/scale_5_gamma:0' shape=(1,) dtype=float32> gamma\n",
            "tracking <tf.Variable 'scale_6/scale_6_gamma:0' shape=(1,) dtype=float32> gamma\n",
            "tracking <tf.Variable 'scale_7/scale_7_gamma:0' shape=(1,) dtype=float32> gamma\n",
            "tracking <tf.Variable 'scale_8/scale_8_gamma:0' shape=(1,) dtype=float32> gamma\n",
            "Error occurs during loading weights files.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4uUub3A4vi7"
      },
      "source": [
        "fd = MTCNNFaceDetector(sess=K.get_session(), model_path=\"./mtcnn_weights/\")\n",
        "vc = VideoConverter()\n",
        "vc.set_face_detector(fd)\n",
        "vc.set_gan_model(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxwjrVoc4vmQ"
      },
      "source": [
        "options = {\n",
        "    # ===== Fixed =====\n",
        "    \"use_smoothed_bbox\": True,\n",
        "    \"use_kalman_filter\": True,\n",
        "    \"use_auto_downscaling\": False,\n",
        "    \"bbox_moving_avg_coef\": 0.65,\n",
        "    \"min_face_area\": 35 * 35,\n",
        "    \"IMAGE_SHAPE\": model.IMAGE_SHAPE,\n",
        "    # ===== Tunable =====\n",
        "    \"kf_noise_coef\": 1e-3,\n",
        "    \"use_color_correction\": \"hist_match\",\n",
        "    \"detec_threshold\": 0.8,\n",
        "    \"roi_coverage\": 0.9,\n",
        "    \"enhance\": 0.,\n",
        "    \"output_type\": 3,\n",
        "    \"direction\": \"AtoB\", # ==================== This line determines the transform direction ====================\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyuELtb94vpr"
      },
      "source": [
        "if options[\"direction\"] == \"AtoB\":\n",
        "    input_fn = fn_source_video\n",
        "    output_fn = \"OUTPUT_VIDEO_AtoB.mp4\"\n",
        "elif options[\"direction\"] == \"BtoA\":\n",
        "    input_fn = fn_target_video\n",
        "    output_fn = \"OUTPUT_VIDEO_BtoA.mp4\"\n",
        "\n",
        "duration = None # None or a non-negative float tuple: (start_sec, end_sec). Duration of input video to be converted"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sem7RMzm4vr6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 749
        },
        "outputId": "5c8e876e-7c0c-4c64-c11a-77b7a8c24c15"
      },
      "source": [
        "vc.convert(input_fn=input_fn, output_fn=output_fn, options=options, duration=duration)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Face alignment error occurs at frame 1.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m/content/faceswap-GAN/converter/video_converter.py\u001b[0m in \u001b[0;36mprocess_video\u001b[0;34m(self, input_img, options)\u001b[0m\n\u001b[1;32m    164\u001b[0m                     \u001b[0mcolor_correction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"use_color_correction\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m                     \u001b[0mIMAGE_SHAPE\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"IMAGE_SHAPE\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m                     )\n",
            "\u001b[0;32m/content/faceswap-GAN/converter/face_transformer.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, inp_img, direction, roi_coverage, color_correction, IMAGE_SHAPE)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;31m# Set 1 member: self.ae_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ae_forward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mae_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/faceswap-GAN/converter/face_transformer.py\u001b[0m in \u001b[0;36m_ae_forward_pass\u001b[0;34m(self, ae_input)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_ae_forward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mae_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mae_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mae_input\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mae_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mae_out\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3476\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: 2 root error(s) found.\n  (0) Invalid argument: transpose expects a vector of size 3. But input(1) is a vector of size 4\n\t [[{{node model_1/conv2d_1/convolution-0-TransposeNHWCToNCHW-LayoutOptimizer}}]]\n  (1) Invalid argument: transpose expects a vector of size 3. But input(1) is a vector of size 4\n\t [[{{node model_1/conv2d_1/convolution-0-TransposeNHWCToNCHW-LayoutOptimizer}}]]\n\t [[concatenate_20/concat/_537]]\n0 successful operations.\n0 derived errors ignored.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-1b358f26260b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mduration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mduration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/faceswap-GAN/converter/video_converter.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, input_fn, output_fn, options, duration)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mclip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfl_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mduration\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mduration\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m             \u001b[0mclip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfl_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0mclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_videofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mclip1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/moviepy/video/VideoClip.py\u001b[0m in \u001b[0;36mfl_image\u001b[0;34m(self, image_func, apply_to)\u001b[0m\n\u001b[1;32m    512\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mapply_to\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m             \u001b[0mapply_to\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mgf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mimage_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_to\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[0;31m# --------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/moviepy/Clip.py\u001b[0m in \u001b[0;36mfl\u001b[0;34m(self, fun, apply_to, keep_duration)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;31m#mf = copy(self.make_frame)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0mnewclip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_make_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkeep_duration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-171>\u001b[0m in \u001b[0;36mset_make_frame\u001b[0;34m(self, mf)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/moviepy/decorators.py\u001b[0m in \u001b[0;36moutplace\u001b[0;34m(f, clip, *a, **k)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;34m\"\"\" Applies f(clip.copy(), *a, **k) and returns clip.copy()\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mnewclip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewclip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnewclip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/moviepy/video/VideoClip.py\u001b[0m in \u001b[0;36mset_make_frame\u001b[0;34m(self, mf)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \"\"\"\n\u001b[1;32m    668\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moutplace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-124>\u001b[0m in \u001b[0;36mget_frame\u001b[0;34m(self, t)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/moviepy/decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(f, *a, **kw)\u001b[0m\n\u001b[1;32m     87\u001b[0m         new_kw = {k: fun(v) if k in varnames else v\n\u001b[1;32m     88\u001b[0m                  for (k,v) in kw.items()}\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnew_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/moviepy/Clip.py\u001b[0m in \u001b[0;36mget_frame\u001b[0;34m(self, t)\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_duration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/moviepy/Clip.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;31m#mf = copy(self.make_frame)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0mnewclip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_make_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkeep_duration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/moviepy/video/VideoClip.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(gf, t)\u001b[0m\n\u001b[1;32m    512\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mapply_to\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m             \u001b[0mapply_to\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mgf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mimage_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_to\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[0;31m# --------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/faceswap-GAN/converter/video_converter.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mclip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfl_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mduration\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mduration\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m             \u001b[0mclip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfl_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0mclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_videofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mclip1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/faceswap-GAN/converter/video_converter.py\u001b[0m in \u001b[0;36mprocess_video\u001b[0;34m(self, input_img, options)\u001b[0m\n\u001b[1;32m    186\u001b[0m                     \u001b[0mroi_coverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"roi_coverage\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                     \u001b[0mcolor_correction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"use_color_correction\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m                     \u001b[0mIMAGE_SHAPE\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"IMAGE_SHAPE\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m                     )\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/faceswap-GAN/converter/face_transformer.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, inp_img, direction, roi_coverage, color_correction, IMAGE_SHAPE)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;31m# model inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;31m# Set 1 member: self.ae_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ae_forward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mae_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;31m# post-process transformed roi image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/faceswap-GAN/converter/face_transformer.py\u001b[0m in \u001b[0;36m_ae_forward_pass\u001b[0;34m(self, ae_input)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_ae_forward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mae_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mae_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mae_input\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mae_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mae_out\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3476\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: 2 root error(s) found.\n  (0) Invalid argument: transpose expects a vector of size 3. But input(1) is a vector of size 4\n\t [[{{node model_1/conv2d_1/convolution-0-TransposeNHWCToNCHW-LayoutOptimizer}}]]\n\t [[concatenate_20/concat/_537]]\n  (1) Invalid argument: transpose expects a vector of size 3. But input(1) is a vector of size 4\n\t [[{{node model_1/conv2d_1/convolution-0-TransposeNHWCToNCHW-LayoutOptimizer}}]]\n0 successful operations.\n0 derived errors ignored."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwKOc5Jt5oFO"
      },
      "source": [
        "# Download result video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0MkktlHC5sh"
      },
      "source": [
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwDUxwlL4voW"
      },
      "source": [
        "if options[\"direction\"] == \"AtoB\":\n",
        "    files.download(\"OUTPUT_VIDEO_AtoB.mp4\")\n",
        "elif options[\"direction\"] == \"BtoA\":\n",
        "    files.download(\"OUTPUT_VIDEO_BtoA.mp4\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Yo2X7VM21Tx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}